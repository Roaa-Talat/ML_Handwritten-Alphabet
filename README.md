# Machine Learning Handwritten Alphabet Recognition

## ğŸ“– Project Overview
This project involves recognizing handwritten English alphabets using machine learning techniques. The dataset consists of grayscale images represented in a structured CSV format, making it ideal for training models in character recognition.

### ğŸ¯ Objectives:
- Load and preprocess the dataset.
- Perform exploratory data analysis (EDA).
- Train and evaluate multiple models:
  - **Support Vector Machine (SVM) - Linear & Non-linear**
  - **Logistic Regression (from scratch)**
  - **Neural Networks (TensorFlow-based)**
- Compare model performance and suggest the best approach.

---

## ğŸ“Š Dataset Overview
ğŸ”— **Dataset Link:** [A-Z Handwritten Alphabets Dataset](https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format)

âœ” **Size:** 370,000+ images of handwritten alphabets.  
âœ” **Format:** CSV file where each row represents an image with pixel values and labels.  
âœ” **Resolution:** 28Ã—28 pixels, grayscale format.  
âœ” **Labels:** 26 classes (A-Z) mapped numerically (0 = A, 1 = B, ..., 25 = Z).  

---

## ğŸ›  Data Preprocessing
### âœ… Steps:

1ï¸âƒ£ **Load and Explore Dataset**
   - Inspect dataset structure and statistics.
   
2ï¸âƒ£ **Preprocess Data**
   - Normalize pixel values to [0,1] range.
   - Reshape flattened vectors into 28Ã—28 images for visualization.
   
3ï¸âƒ£ **Split Data**
   - Training (80%) / Testing (20%) using `train_test_split`.
   
4ï¸âƒ£ **Visualize Sample Images**
   - Display random samples to check data quality.

---

## ğŸ¤– Machine Learning Models
### ğŸ“Œ 1. **Support Vector Machine (SVM)**
âœ” Implemented both **Linear SVM** and **Non-Linear SVM (RBF Kernel)**.
âœ” SVM provided high accuracy but required careful kernel tuning.

### ğŸ“Œ 2. **Logistic Regression (from Scratch)**
âœ” Implemented one-vs-all multi-class classification.
âœ” Tested different learning rates (`0.1`, `0.01`, `0.5`).
âœ” Achieved high accuracy but was computationally expensive.

### ğŸ“Œ 3. **Neural Networks (TensorFlow-based)**
âœ” Implemented a deep learning model with:
   - Input Layer: Flatten(28Ã—28)
   - Hidden Layers: Fully connected (ReLU activation, Dropout)
   - Output Layer: Softmax (26 classes)
âœ” Optimized model achieved **98% accuracy**.

---

## ğŸ“Š Model Performance Comparison
| Model                | Test Accuracy | Training Time |
|----------------------|--------------|--------------|
| **SVM (Linear)**    | 88.98%       | Moderate    |
| **SVM (Non-Linear)**| 94.79%       | High        |
| **Logistic Regression** | 85.00% | Very High   |
| **Neural Network (Model 1)** | 97.00% | Moderate |
| **Neural Network (Optimized)** | **98.00%** | High |

---

## ğŸš€ Key Takeaways
âœ… **Neural Network (Optimized)** outperformed all models in accuracy.  

âœ… **SVM (Non-Linear)** was an excellent alternative but required longer training time.  

âœ… **Logistic Regression** was interpretable but computationally expensive.  

âœ… **Data preprocessing significantly impacts final model accuracy.**  

---

## ğŸ›  Technologies & Libraries Used
ğŸ”¹ **Python**  
ğŸ”¹ **Pandas, NumPy** (Data manipulation)  
ğŸ”¹ **Matplotlib, Seaborn** (Data visualization)  
ğŸ”¹ **Scikit-learn** (SVM, Logistic Regression)  
ğŸ”¹ **TensorFlow, Keras** (Deep Learning)  

---

## ğŸ”® Future Enhancements
ğŸ”¹ Implement **CNNs (Convolutional Neural Networks)** for better feature extraction.  
ğŸ”¹ Experiment with **data augmentation** to improve generalization.  
ğŸ”¹ Optimize SVM with **different kernel functions and hyperparameters**.  

---

### ğŸ“Œ **Authors: Team Members (Cairo University)**

